diff --git a/packages/Cartopy/meta.yaml b/packages/Cartopy/meta.yaml
new file mode 100644
index 00000000..e9f45ec2
--- /dev/null
+++ b/packages/Cartopy/meta.yaml
@@ -0,0 +1,44 @@
+package:
+  name: Cartopy
+  version: 0.21.1
+  top-level:
+    - cartopy
+source:
+  url: https://files.pythonhosted.org/packages/e8/11/ed3e364b3910f0951821e6b5a03a03ce425464b72aa3da08d47b78ae17bd/Cartopy-0.21.1.tar.gz
+  sha256: 89d5649712c8582231c6e11825a04c85f6f0cee94dbb89e4db23eabca1cc250a
+  patches:
+    - patches/0000-geos-config-pre-existing-data.patch
+test:
+  imports:
+    - cartopy
+    - cartopy.trace
+    - cartopy.mpl.geoaxes
+    - cartopy.crs
+requirements:
+  host:
+    - geos
+    - numpy
+  run:
+    - shapely
+    - pyshp
+    - pyproj
+    - geos
+    - matplotlib
+    - scipy
+
+build:
+  vendor-sharedlib: true
+  script: |
+    export GEOS_CONFIG=${WASM_LIBRARY_DIR}/bin/geos-config
+    echo ${GEOS_CONFIG}
+    mkdir -p lib/cartopy/data/shapefiles/natural_earth/physical
+    wget https://naturalearth.s3.amazonaws.com/110m_physical/ne_110m_coastline.zip -O lib/cartopy/data/shapefiles/natural_earth/physical/ne_110m_coastline.zip
+    unzip lib/cartopy/data/shapefiles/natural_earth/physical/ne_110m_coastline.zip -d lib/cartopy/data/shapefiles/natural_earth/physical/
+    rm -f lib/cartopy/data/shapefiles/natural_earth/physical/ne_110m_coastline.zip
+    rm -f lib/cartopy/data/shapefiles/natural_earth/physical/ne_110m_coastline.README.html
+
+about:
+  home: http://scitools.org.uk/cartopy
+  PyPI: https://pypi.org/project/Cartopy/
+  summary: A library providing cartographic tools for python
+  license: LGPL-3.0-or-later
diff --git a/packages/Cartopy/patches/0000-geos-config-pre-existing-data.patch b/packages/Cartopy/patches/0000-geos-config-pre-existing-data.patch
new file mode 100644
index 00000000..9dddd14c
--- /dev/null
+++ b/packages/Cartopy/patches/0000-geos-config-pre-existing-data.patch
@@ -0,0 +1,43 @@
+diff --git a/lib/cartopy/__init__.py b/lib/cartopy/__init__.py
+index d2bd7e8c..ef496f4a 100644
+--- a/lib/cartopy/__init__.py
++++ b/lib/cartopy/__init__.py
+@@ -21,7 +21,7 @@ _data_dir = os.path.join(os.environ.get("XDG_DATA_HOME", _writable_dir),
+                          'cartopy')
+ _cache_dir = os.path.join(tempfile.gettempdir(), 'cartopy_cache_dir')
+ 
+-config = {'pre_existing_data_dir': os.environ.get('CARTOPY_DATA_DIR', ''),
++config = {'pre_existing_data_dir': os.path.join(os.path.dirname(__file__), 'data'),
+           'data_dir': _data_dir,
+           'cache_dir': _cache_dir,
+           'repo_data_dir': os.path.join(os.path.dirname(__file__), 'data'),
+diff --git a/setup.py b/setup.py
+index 8e061479..0a2b4344 100644
+--- a/setup.py
++++ b/setup.py
+@@ -81,11 +81,12 @@ def file_walk_relative(top, remove=''):
+ 
+ # GEOS
+ try:
+-    geos_version = subprocess.check_output(['geos-config', '--version'])
++    geos_config = os.environ.get("GEOS_CONFIG", "/src/packages/.libs/bin/geos-config")
++    geos_version = subprocess.check_output([geos_config, '--version'])
+     geos_version = tuple(int(v) for v in geos_version.split(b'.')
+                          if 'dev' not in str(v))
+-    geos_includes = subprocess.check_output(['geos-config', '--includes'])
+-    geos_clibs = subprocess.check_output(['geos-config', '--clibs'])
++    geos_includes = subprocess.check_output([geos_config, '--includes'])
++    geos_clibs = subprocess.check_output([geos_config, '--clibs'])
+ except (OSError, ValueError, subprocess.CalledProcessError):
+     warnings.warn(
+         'Unable to determine GEOS version. Ensure you have %s or later '
+@@ -236,6 +237,9 @@ setup(
+                   list(file_walk_relative('lib/cartopy/data/'
+                                           'shapefiles/gshhs',
+                                           remove='lib/cartopy/')) +
++                  list(file_walk_relative('lib/cartopy/data/'
++                                          'shapefiles/natural_earth',
++                                          remove='lib/cartopy/')) +
+                   list(file_walk_relative('lib/cartopy/tests/lakes_shapefile',
+                                           remove='lib/cartopy/')) +
+                   ['io/srtm.npz']},
diff --git a/packages/Cartopy/test_cartopy.py b/packages/Cartopy/test_cartopy.py
new file mode 100644
index 00000000..6dfc8934
--- /dev/null
+++ b/packages/Cartopy/test_cartopy.py
@@ -0,0 +1,25 @@
+import pathlib
+
+import pytest
+from pytest_pyodide import run_in_pyodide
+
+
+@pytest.mark.driver_timeout(60)
+@run_in_pyodide(packages=["cartopy"])
+def test_imports(selenium):
+    import cartopy
+    import cartopy.trace
+
+    assert False
+
+
+@pytest.mark.driver_timeout(60)
+@run_in_pyodide(packages=["cartopy", "matplotlib"])
+def test_matplotlib(selenium):
+    import cartopy.crs as ccrs
+    import matplotlib.pyplot as plt
+
+    ax = plt.axes(projection=ccrs.PlateCarree())
+    ax.coastlines()
+
+    plt.show()
diff --git a/packages/cfgrib/meta.yaml b/packages/cfgrib/meta.yaml
new file mode 100644
index 00000000..735c4ef0
--- /dev/null
+++ b/packages/cfgrib/meta.yaml
@@ -0,0 +1,22 @@
+package:
+  name: cfgrib
+  version: 0.9.10.4
+  top-level:
+    - cfgrib
+source:
+  url: https://files.pythonhosted.org/packages/0d/61/2b152f062ffa494ce57ee5befe025955f4bb245f4ceeb78f1a682be65438/cfgrib-0.9.10.4-py3-none-any.whl
+  sha256: 563416811cd53a861acf47998ef21769accba641b1715d2af4d7e165c4868beb
+requirements:
+  run:
+    - attrs
+    - click
+    - eccodes
+    - numpy
+    - xarray
+about:
+  home: https://github.com/ecmwf/cfgrib
+  PyPI: https://pypi.org/project/cfgrib
+  summary:
+    Python interface to map GRIB files to the NetCDF Common Data Model following
+    the CF Convention using ecCodes.
+  license: Apache License Version 2.0
diff --git a/packages/dask/meta.yaml b/packages/dask/meta.yaml
new file mode 100644
index 00000000..72e2ee63
--- /dev/null
+++ b/packages/dask/meta.yaml
@@ -0,0 +1,25 @@
+package:
+  name: dask
+  version: 2023.5.0
+  top-level:
+    - dask
+source:
+  url: https://files.pythonhosted.org/packages/07/93/32d3e317fec6d0fc130284f922ad9bd13d9ae0d52245e6ff6e57647e924c/dask-2023.5.0-py3-none-any.whl
+  sha256: 32b34986519b7ddc0947c8ca63c2fc81b964e4c208dfb5cbf9f4f8aec92d152b
+test:
+  imports:
+    - dask
+requirements:
+  run:
+    - click
+    - cloudpickle
+    - importlib_metadata
+    - Jinja2
+    - packaging
+    - pyyaml
+    - toolz
+about:
+  home: https://github.com/dask/dask/
+  PyPI: https://pypi.org/project/dask
+  summary: Parallel PyData with Task Scheduling
+  license: BSD
diff --git a/packages/eccodes/meta.yaml b/packages/eccodes/meta.yaml
new file mode 100644
index 00000000..791a9f52
--- /dev/null
+++ b/packages/eccodes/meta.yaml
@@ -0,0 +1,29 @@
+package:
+  name: eccodes
+  version: 1.5.2
+  top-level:
+    - eccodes
+    - gribapi
+source:
+  url: https://files.pythonhosted.org/packages/af/b9/57d55d70ca6fbf9ce8dc096904d6bd4984d6eaacfff20c771245d99e1f1b/eccodes-1.5.2.tar.gz
+  sha256: f7cce47fc9b1df3ed9eea21c4060fa572e07a4d0f014f6fd1f74683df9b45801
+  patches:
+    - patches/0001-shared-library-include.patch
+requirements:
+  run:
+    - attrs
+    - cffi
+    - findlibs
+    - numpy
+  host:
+    - libeccodes
+build:
+  vendor-sharedlib: true
+  script: |
+    cp ${WASM_LIBRARY_DIR}/lib/libeccodes.so gribapi/
+    cp -r ${WASM_LIBRARY_DIR}/share/eccodes/definitions gribapi/
+about:
+  home: https://github.com/ecmwf/eccodes-python
+  PyPI: https://pypi.org/project/eccodes
+  summary: Python interface to the ecCodes GRIB and BUFR decoder/encoder
+  license: Apache License Version 2.0
diff --git a/packages/eccodes/patches/0001-shared-library-include.patch b/packages/eccodes/patches/0001-shared-library-include.patch
new file mode 100644
index 00000000..671f423a
--- /dev/null
+++ b/packages/eccodes/patches/0001-shared-library-include.patch
@@ -0,0 +1,45 @@
+diff --git a/gribapi/bindings.py b/gribapi/bindings.py
+index 91f6d6a..2504873 100644
+--- a/gribapi/bindings.py
++++ b/gribapi/bindings.py
+@@ -15,8 +15,10 @@
+ #
+ 
+ from __future__ import absolute_import, division, print_function, unicode_literals
++from pathlib import Path
+ 
+ import logging
++import os
+ import pkgutil
+ 
+ import cffi
+@@ -25,14 +27,10 @@ __version__ = "1.6.0"
+ 
+ LOG = logging.getLogger(__name__)
+ 
+-try:
+-    import ecmwflibs as findlibs
+-except ImportError:
+-    import findlibs
++definitions_path = str(Path(__file__).parent / "definitions")
++os.environ["ECCODES_DEFINITION_PATH"] = definitions_path
+ 
+-library_path = findlibs.find("eccodes")
+-if library_path is None:
+-    raise RuntimeError("Cannot find the ecCodes library")
++library_path = str(Path(__file__).parent / "libeccodes.so")
+ 
+ # default encoding for ecCodes strings
+ ENC = "ascii"
+diff --git a/setup.py b/setup.py
+index 5df68df..d58dd3a 100644
+--- a/setup.py
++++ b/setup.py
+@@ -55,6 +55,7 @@ setuptools.setup(
+     url="https://github.com/ecmwf/eccodes-python",
+     packages=setuptools.find_packages(),
+     include_package_data=True,
++    package_data={'gribapi': ['libeccodes.so', "definitions/*", "definitions/**/*"]},
+     install_requires=install_requires,
+     tests_require=[
+         "pytest",
diff --git a/packages/fcpy/fcpy/LICENSE.txt b/packages/fcpy/fcpy/LICENSE.txt
new file mode 100644
index 00000000..3ef8c129
--- /dev/null
+++ b/packages/fcpy/fcpy/LICENSE.txt
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright 2022 ECMWF
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/packages/fcpy/fcpy/fcpy/__init__.py b/packages/fcpy/fcpy/fcpy/__init__.py
new file mode 100644
index 00000000..f0bc7a0b
--- /dev/null
+++ b/packages/fcpy/fcpy/fcpy/__init__.py
@@ -0,0 +1,21 @@
+# (C) Copyright 2022 ECMWF.
+#
+# This software is licensed under the terms of the Apache Licence Version 2.0
+# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
+# In applying this licence, ECMWF does not waive the privileges and immunities
+# granted to it by virtue of its status as an intergovernmental organisation
+# nor does it submit to any jurisdiction.
+#
+
+# Note: metview has to be imported first to avoid loading errors.
+# try:
+#     import metview
+# except:
+#     pass
+
+from .compressors import *  # noqa
+from .datasets import *  # noqa
+from .metrics import *  # noqa
+from .sigma import *  # noqa
+from .suite import *  # noqa
+from .utils import *  # noqa
diff --git a/packages/fcpy/fcpy/fcpy/compressors.py b/packages/fcpy/fcpy/fcpy/compressors.py
new file mode 100644
index 00000000..89e698a6
--- /dev/null
+++ b/packages/fcpy/fcpy/fcpy/compressors.py
@@ -0,0 +1,217 @@
+# (C) Copyright 2022 ECMWF.
+#
+# This software is licensed under the terms of the Apache Licence Version 2.0
+# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
+# In applying this licence, ECMWF does not waive the privileges and immunities
+# granted to it by virtue of its status as an intergovernmental organisation
+# nor does it submit to any jurisdiction.
+#
+
+from abc import ABCMeta, abstractmethod
+from typing import Optional, Tuple
+
+import numpy as np
+
+# from .utils import compute_min_bits, get_bits_params
+
+
+class Compressor(metaclass=ABCMeta):
+    """Abstract base class for compressors.
+
+    Args:
+        inner_compressor (Compressor, optional): Inner compressor to use, if any.
+        bits (int, optional): Bits to use for compression. If None, must be given
+            when calling compress().
+    """
+
+    def __init__(
+        self,
+        inner_compressor: Optional["Compressor"] = None,
+        bits: Optional[int] = None,
+    ):
+        if inner_compressor is not None:
+            assert isinstance(inner_compressor, Compressor)
+        self.inner_compressor = inner_compressor
+        self.bits = bits
+
+    @property
+    def name(self) -> str:
+        """Name of the compressor."""
+        s = type(self).__name__
+        if self.inner_compressor:
+            s = f"{s}({self.inner_compressor.name})"
+        return s
+
+    def compress(
+        self, arr: np.ndarray, bits: Optional[int] = None
+    ) -> Tuple[np.ndarray, list[dict]]:
+        """Compress the given array.
+
+        Args:
+            arr (np.ndarray): Data to compress.
+            bits (int, optional): Bits to use in compression.
+                Defaults to bits passed in the constructor.
+
+        Returns:
+            Tuple[np.ndarray, list[dict]]:
+                The compressed data with metadata of this and all
+                inner compressors required for decompression.
+        """
+        bits = bits or self.bits
+        assert bits is not None
+        if self.inner_compressor:
+            arr, params_stack = self.inner_compressor.compress(arr, bits=bits)
+        else:
+            params_stack = []
+        c, params = self.do_compress(arr, bits)
+        params_stack.insert(0, params)
+        return c, params_stack
+
+    def decompress(
+        self, compressed_data: np.ndarray, params_stack: list[dict]
+    ) -> np.ndarray:
+        """Decompress the given data using the given compression metadata.
+
+        Args:
+            compressed_data (np.ndarray): The compressed data.
+            params_stack (list[dict]): The compression metadata of this
+                and all inner compressors.
+
+        Returns:
+            np.ndarray: The decompressed data.
+        """
+        params = params_stack.pop(0)
+        d = self.do_decompress(compressed_data, params)
+        if self.inner_compressor:
+            d = self.inner_compressor.decompress(d, params_stack)
+        return d
+
+    @abstractmethod
+    def do_compress(self, arr: np.ndarray, bits: int) -> Tuple[np.ndarray, dict]:
+        """Method to be implemented in compressor subclasses.
+        Called by the base class during compress().
+
+        Args:
+            arr (np.ndarray): Data to compress.
+            bits (int): Bits to use for compression.
+
+        Returns:
+            Tuple[np.ndarray, dict]: Compressed data with metadata required for decompression.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def do_decompress(self, compressed_data: np.ndarray, params: dict) -> np.ndarray:
+        """Method to be implemented in compressor subclasses.
+        Called by the base class during decompress().
+
+        Args:
+            compressed_data (np.ndarray): The compressed data.
+            params (dict): The compression metadata returned by do_compress().
+
+        Returns:
+            np.ndarray: The decompressed data.
+        """
+        raise NotImplementedError
+
+
+class LinQuantization(Compressor):
+    """Linear quantization compressor."""
+
+    def do_compress(self, arr: np.ndarray, bits: int) -> Tuple[np.ndarray, dict]:
+        minimum = np.amin(arr)
+        maximum = np.amax(arr)
+        arr_compressed = np.round(
+            (arr - minimum) / (maximum - minimum) * (2**bits - 1)
+        )
+        return arr_compressed, {"bits": bits, "minimum": minimum, "maximum": maximum}
+
+    def do_decompress(self, compressed_data: np.ndarray, params: dict) -> np.ndarray:
+        bits = params["bits"]
+        minimum = params["minimum"]
+        maximum = params["maximum"]
+        arr_decompressed = (
+            np.array(compressed_data) / (2**bits - 1) * (maximum - minimum) + minimum
+        )
+        return arr_decompressed
+
+
+# class Round(Compressor):
+#     """Rounding compressor."""
+
+#     def get_used_sign_and_exponent_bits(self, arr: np.ndarray) -> int:
+#         bits_params = get_bits_params(arr)
+#         used_sign_and_exponent_bits = compute_min_bits(arr, bits_params)
+
+#         return used_sign_and_exponent_bits
+
+#     def do_compress(self, arr: np.ndarray, bits: int) -> Tuple[np.ndarray, dict]:
+#         from .julia import BitInformation
+
+#         used_sign_and_exponent_bits = self.get_used_sign_and_exponent_bits(arr)
+#         mantissa_bits = bits - used_sign_and_exponent_bits
+#         if mantissa_bits < 1:
+#             raise RuntimeError("Round: mantissa bits < 1, use higher bits value")
+#         return BitInformation.round(arr, mantissa_bits), {}
+
+#     def do_decompress(self, compressed_data: np.ndarray, params: dict) -> np.ndarray:
+#         return compressed_data
+
+
+class Float(Compressor):
+    """IEEE Floating-Point compressor."""
+
+    DTYPE = {16: np.float16, 32: np.float32, 64: np.float64}
+
+    def do_compress(self, arr: np.ndarray, bits: int) -> Tuple[np.ndarray, dict]:
+        dtype = self.DTYPE[bits]
+        return arr.astype(dtype, copy=False), {}
+
+    def do_decompress(self, compressed_data: np.ndarray, params: dict) -> np.ndarray:
+        return compressed_data
+
+
+class Log(Compressor):
+    """Log/Exp compressor, typically used for pre-/postprocessing."""
+
+    def do_compress(self, arr: np.ndarray, bits: int) -> Tuple[np.ndarray, dict]:
+        return np.log1p(arr), {}
+
+    def do_decompress(self, compressed_data: np.ndarray, params: dict) -> np.ndarray:
+        return np.expm1(compressed_data)
+
+
+class Identity(Compressor):
+    """Identity compressor, performs f(x)=x, i.e. no compression."""
+
+    def do_compress(self, arr: np.ndarray, bits: int) -> Tuple[np.ndarray, dict]:
+        return arr, {}
+
+    def do_decompress(self, compressed_data: np.ndarray, params: dict) -> np.ndarray:
+        return compressed_data
+
+
+class PCA(Compressor):
+    """Principal Component Analysis compressor."""
+
+    def do_compress(self, arr: np.ndarray, bits: int) -> Tuple[np.ndarray, dict]:
+        import sklearn
+        from sklearn.decomposition import PCA
+
+        time, lev, lat, lon = arr.shape
+
+        data = arr.reshape((time * lev, lat * lon))
+
+        transformer = PCA(random_state=42, n_components=bits)
+        compressed_data = transformer.fit_transform(data)
+
+        return compressed_data, {"transformer": transformer, "shape": arr.shape}
+
+    def do_decompress(self, compressed_data: np.ndarray, params: dict) -> np.array:
+        time, lev, lat, lon = params["shape"]
+
+        return (
+            params["transformer"]
+            .inverse_transform(compressed_data)
+            .reshape((time, lev, lat, lon))
+        )
diff --git a/packages/fcpy/fcpy/fcpy/datasets.py b/packages/fcpy/fcpy/fcpy/datasets.py
new file mode 100644
index 00000000..7a5b6505
--- /dev/null
+++ b/packages/fcpy/fcpy/fcpy/datasets.py
@@ -0,0 +1,420 @@
+# (C) Copyright 2022 ECMWF.
+#
+# This software is licensed under the terms of the Apache Licence Version 2.0
+# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
+# In applying this licence, ECMWF does not waive the privileges and immunities
+# granted to it by virtue of its status as an intergovernmental organisation
+# nor does it submit to any jurisdiction.
+#
+
+import glob
+
+# import climetlab as cml
+import xarray as xr
+import cfgrib # TODO: remove
+
+# DATASET_ID = "climetlab-fields-compression"
+
+# PARAMETERS = {
+#     "atmospheric-model": {
+#         "ml": {
+#             "anoffset": 9,
+#             "class": "rd",
+#             "date": "2020-07-21",
+#             "expver": "hplp",
+#             "levelist": [
+#                 1,
+#                 2,
+#                 3,
+#                 4,
+#                 5,
+#                 6,
+#                 7,
+#                 8,
+#                 9,
+#                 10,
+#                 11,
+#                 12,
+#                 13,
+#                 14,
+#                 15,
+#                 16,
+#                 17,
+#                 18,
+#                 19,
+#                 20,
+#                 21,
+#                 22,
+#                 23,
+#                 24,
+#                 25,
+#                 26,
+#                 27,
+#                 28,
+#                 29,
+#                 30,
+#                 31,
+#                 32,
+#                 33,
+#                 34,
+#                 35,
+#                 36,
+#                 37,
+#                 38,
+#                 39,
+#                 40,
+#                 41,
+#                 42,
+#                 43,
+#                 44,
+#                 45,
+#                 46,
+#                 47,
+#                 48,
+#                 49,
+#                 50,
+#                 51,
+#                 52,
+#                 53,
+#                 54,
+#                 55,
+#                 56,
+#                 57,
+#                 58,
+#                 59,
+#                 60,
+#                 61,
+#                 62,
+#                 63,
+#                 64,
+#                 65,
+#                 66,
+#                 67,
+#                 68,
+#                 69,
+#                 70,
+#                 71,
+#                 72,
+#                 73,
+#                 74,
+#                 75,
+#                 76,
+#                 77,
+#                 78,
+#                 79,
+#                 80,
+#                 81,
+#                 82,
+#                 83,
+#                 84,
+#                 85,
+#                 86,
+#                 87,
+#                 88,
+#                 89,
+#                 90,
+#                 91,
+#                 92,
+#                 93,
+#                 94,
+#                 95,
+#                 96,
+#                 97,
+#                 98,
+#                 99,
+#                 100,
+#                 101,
+#                 102,
+#                 103,
+#                 104,
+#                 105,
+#                 106,
+#                 107,
+#                 108,
+#                 109,
+#                 110,
+#                 111,
+#                 112,
+#                 113,
+#                 114,
+#                 115,
+#                 116,
+#                 117,
+#                 118,
+#                 119,
+#                 120,
+#                 121,
+#                 122,
+#                 123,
+#                 124,
+#                 125,
+#                 126,
+#                 127,
+#                 128,
+#                 129,
+#                 130,
+#                 131,
+#                 132,
+#                 133,
+#                 134,
+#                 135,
+#                 136,
+#                 137,
+#             ],
+#             "levtype": "ml",
+#             "param": [
+#                 "cc",
+#                 "ciwc",
+#                 "clwc",
+#                 "crwc",
+#                 "cswc",
+#                 "o3",
+#                 "q",
+#                 # unsupported:
+#                 # "lnsp",
+#                 # "d",
+#                 # "t",
+#                 # "vo",
+#                 # "w",
+#                 # "z",
+#                 # "u",
+#                 # "v"
+#             ],
+#             "step": [
+#                 0,
+#                 12,
+#                 24,
+#                 36,
+#                 48,
+#                 60,
+#                 72,
+#                 84,
+#                 96,
+#                 108,
+#                 120,
+#                 132,
+#                 144,
+#                 156,
+#                 168,
+#                 180,
+#                 192,
+#                 204,
+#                 216,
+#                 228,
+#                 240,
+#             ],
+#             "stream": "lwda",
+#             "time": "00:00:00",
+#             "type": "fc",
+#         },
+#         "sfc": {
+#             "anoffset": 9,
+#             "class": "rd",
+#             "date": "2020-07-21",
+#             "expver": "hplp",
+#             "levtype": "sfc",
+#             "param": [
+#                 "10fg",
+#                 "10u",
+#                 "10v",
+#                 "2d",
+#                 "2t",
+#                 "asn",
+#                 "bld",
+#                 "blh",
+#                 "cape",
+#                 "chnk",
+#                 "ci",
+#                 "cin",
+#                 "cp",
+#                 "crr",
+#                 "csfr",
+#                 "dsrp",
+#                 "e",
+#                 "es",
+#                 "ewss",
+#                 "fal",
+#                 "flsr",
+#                 "fsr",
+#                 "fzra",
+#                 "gwd",
+#                 "hcc",
+#                 "i10fg",
+#                 "ie",
+#                 "iews",
+#                 # Ambiguous : ilspf could be INSTANTANEOUS SURFACE SENSIBLE HEAT FLUX or INSTANTANEOUS 10 METRE WIND GUST
+#                 # "ilspf",
+#                 "inss",
+#                 "ishf",
+#                 "istl1",
+#                 "istl2",
+#                 "istl3",
+#                 "istl4",
+#                 "kx",
+#                 "lcc",
+#                 "lgws",
+#                 "lsm",
+#                 "lsp",
+#                 "lspf",
+#                 "lsrr",
+#                 "lssfr",
+#                 "mcc",
+#                 "mgws",
+#                 "mn2t",
+#                 "msl",
+#                 "mx2t",
+#                 "nsss",
+#                 "ocu",
+#                 "ocv",
+#                 "par",
+#                 "pev",
+#                 "ptype",
+#                 "ro",
+#                 "rsn",
+#                 "sd",
+#                 "sf",
+#                 "skt",
+#                 "slhf",
+#                 "smlt",
+#                 "src",
+#                 "sro",
+#                 "sshf",
+#                 "ssr",
+#                 "ssrc",
+#                 "ssrd",
+#                 "ssro",
+#                 "sst",
+#                 "stl1",
+#                 "stl2",
+#                 "stl3",
+#                 "stl4",
+#                 "str",
+#                 "strc",
+#                 "strd",
+#                 "sund",
+#                 "swvl1",
+#                 "swvl2",
+#                 "swvl3",
+#                 "swvl4",
+#                 "tcc",
+#                 "tciw",
+#                 "tclw",
+#                 "tco3",
+#                 "tcrw",
+#                 "tcsw",
+#                 "tcw",
+#                 "tcwv",
+#                 "tisr",
+#                 "totalx",
+#                 "tp",
+#                 "tsn",
+#                 "tsr",
+#                 "tsrc",
+#                 "ttr",
+#                 "ttrc",
+#                 "uvb",
+#                 "vimd",
+#                 # Ambiguous : vis could be VERTICAL INTEGRAL OF NORTHWARD OZONE FLUX or VERTICALLY INTEGRATED MOISTURE DIVERGENCE
+#                 # "vis",
+#                 "z",
+#             ],
+#             "step": [
+#                 0,
+#                 12,
+#                 24,
+#                 36,
+#                 48,
+#                 60,
+#                 72,
+#                 84,
+#                 96,
+#                 108,
+#                 120,
+#                 132,
+#                 144,
+#                 156,
+#                 168,
+#                 180,
+#                 192,
+#                 204,
+#                 216,
+#                 228,
+#                 240,
+#             ],
+#             "stream": "lwda",
+#             "time": "00:00:00",
+#             "type": "fc",
+#         },
+#     },
+# }
+
+# import climetlab_fields_compression.main
+
+
+# def no_validation(data, model, levtype, levels, param, step):
+#     pass
+
+
+# climetlab_fields_compression.main.validate_mars_request = no_validation
+
+
+# def flatten(lst):
+#     return [item for sublist in lst for item in sublist]
+
+
+# def fetch_mars_dataset(request: dict, skip_xarray=False) -> xr.Dataset:
+#     """Load a dataset via MARS.
+
+#     Args:
+#         request (dict): MARS request object.
+
+#     Returns:
+#         xr.Dataset: The loaded dataset.
+#     """
+#     dataset = cml.load_source("mars", request)
+#     if skip_xarray:
+#         return dataset
+#     errors = []
+#     for d in dataset:
+#         grid_type = d.metadata("gridType")
+#         if grid_type != "reduced_gg":
+#             var_name = d.field_metadata()["shortName"]
+#             errors.append(f"Grid type not supported: {grid_type} for {var_name}")
+#     if errors:
+#         raise ValueError(". ".join(errors))
+#     out = dataset.to_xarray()
+#     # Store path for regridding, see utils.py.
+#     out.attrs["path"] = dataset[0].path
+#     return out
+
+
+def open_dataset(filepath) -> xr.Dataset:
+    """Open a dataset from one or more local files.
+
+    Args:
+        filepath (str): .grib, may contain wildcards.
+
+    Returns:
+        xr.Dataset: The loaded dataset.
+    """
+    # """Open a dataset from one or more local files.
+
+    # Args:
+    #     filepath (str): Either .nc or .grib, may contain wildcards.
+
+    # Returns:
+    #     xr.Dataset: The loaded dataset.
+    # """
+    # if filepath.endswith(".nc"):
+    #     return xr.open_mfdataset(filepath)
+    # else:
+    #     ds = cml.load_source("file", filepath).to_xarray(
+    #         xarray_open_dataset_kwargs=dict(cache=False)
+    #     )
+    #     path = glob.glob(filepath)[0]
+    #     ds.attrs["path"] = path
+    #     return ds
+    ds = xr.open_mfdataset(filepath, cache=False)
+    path = glob.glob(filepath)[0]
+    ds.attrs["path"] = path
+    return ds
diff --git a/packages/fcpy/fcpy/fcpy/field.py b/packages/fcpy/fcpy/fcpy/field.py
new file mode 100644
index 00000000..de0835c6
--- /dev/null
+++ b/packages/fcpy/fcpy/fcpy/field.py
@@ -0,0 +1,37 @@
+# # (C) Copyright 2022 ECMWF.
+# #
+# # This software is licensed under the terms of the Apache Licence Version 2.0
+# # which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
+# # In applying this licence, ECMWF does not waive the privileges and immunities
+# # granted to it by virtue of its status as an intergovernmental organisation
+# # nor does it submit to any jurisdiction.
+# #
+
+# import numpy as np
+# import xarray as xr
+
+
+# def compute_bitinformation_single(da: xr.DataArray) -> xr.DataArray:
+#     from .julia import BitInformation
+
+#     vals = da.values.flatten()
+#     bitinf = BitInformation.bitinformation(vals)
+#     bitinf_da = xr.DataArray(data=bitinf, dims=["bit"], name="bitinf")
+#     return bitinf_da
+
+
+# def compute_required_bits_for_bitinf(
+#     bitinf: xr.DataArray, information_content: float
+# ) -> int:
+#     bitinf = bitinf.values
+#     if len(bitinf) == 64:
+#         sign_exponent_bits = 12
+#     elif len(bitinf) == 32:
+#         sign_exponent_bits = 9
+#     else:
+#         raise NotImplementedError("unsupported dtype")
+#     required_mantissa_bits = (
+#         np.argmax(np.cumsum(bitinf) / np.sum(bitinf) >= information_content)
+#         # - sign_exponent_bits
+#     )
+#     return required_mantissa_bits
diff --git a/packages/fcpy/fcpy/fcpy/metrics.py b/packages/fcpy/fcpy/fcpy/metrics.py
new file mode 100644
index 00000000..04315885
--- /dev/null
+++ b/packages/fcpy/fcpy/fcpy/metrics.py
@@ -0,0 +1,75 @@
+# (C) Copyright 2022 ECMWF.
+#
+# This software is licensed under the terms of the Apache Licence Version 2.0
+# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
+# In applying this licence, ECMWF does not waive the privileges and immunities
+# granted to it by virtue of its status as an intergovernmental organisation
+# nor does it submit to any jurisdiction.
+#
+
+from abc import ABCMeta, abstractmethod
+
+import numpy as np
+
+
+class Metric(metaclass=ABCMeta):
+    """Base class for Metric subclasses."""
+
+    name: str
+    """Name of the metric.
+    """
+
+    @abstractmethod
+    def compute(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:
+        """Compute the metric with the given reference and computed data.
+
+        Args:
+            x (np.ndarray): Reference data.
+            y (np.ndarray): Computed data.
+
+        Returns:
+            np.ndarray: The values computed by the metric.
+        """
+        raise NotImplementedError
+
+
+class Difference(Metric):
+    """Difference metric: x - y."""
+
+    name = "difference"
+
+    def compute(self, x, y):
+        return difference(x, y)
+
+
+class RelativeError(Metric):
+    """Relative error metric: |x - y| / |max(x) - min(x)|"""
+
+    name = "relative error"
+
+    def compute(self, x, y):
+        return relative_error(x, y)
+
+
+class AbsoluteError(Metric):
+    """Absolute error metric: |x - y|"""
+
+    name = "absolute error"
+
+    def compute(self, x, y):
+        return absolute_error(x, y)
+
+
+METRICS = [Difference, RelativeError, AbsoluteError]
+
+
+def difference(x: np.ndarray, y: np.ndarray) -> np.ndarray:
+    return x - y
+
+
+def absolute_error(x: np.ndarray, y: np.ndarray) -> np.ndarray:
+    return np.abs(x - y)
+
+
+def relative_error(x: np.ndarray, y: np.ndarray) -> np.ndarray:
+    return np.abs(x - y) / np.abs(np.max(x) - np.min(x))
diff --git a/packages/fcpy/fcpy/fcpy/sigma.py b/packages/fcpy/fcpy/fcpy/sigma.py
new file mode 100644
index 00000000..4dacdf5c
--- /dev/null
+++ b/packages/fcpy/fcpy/fcpy/sigma.py
@@ -0,0 +1,244 @@
+# (C) Copyright 2022 ECMWF.
+#
+# This software is licensed under the terms of the Apache Licence Version 2.0
+# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
+# In applying this licence, ECMWF does not waive the privileges and immunities
+# granted to it by virtue of its status as an intergovernmental organisation
+# nor does it submit to any jurisdiction.
+#
+
+import math
+from itertools import product
+
+import matplotlib.pyplot as plt
+import numpy as np
+import xarray as xr
+from kneed import KneeLocator
+from tqdm import tqdm
+
+# import fcpy
+
+# from .suite import run_compressor_single
+from .utils import compute_min_bits, compute_z_score, get_bits_params
+
+
+# @np.errstate(invalid="ignore")
+# def compute_sigmas(da: xr.DataArray, compressors: list) -> xr.DataArray:
+#     """Computes a DataArray of sigmas based on a random uniform-distributed noise field.
+
+#     Args:
+#         da (xr.DataArray): Reference data.
+#         compressors (list): List of compressors.
+
+#     Returns:
+#         xr.DataArray: Sigma values.
+#     """
+
+#     # https://stackoverflow.com/a/51052046/8893833
+#     import numpy as np
+#     from skimage.restoration import estimate_sigma
+
+#     # Compute a pseudo random as reference
+#     arr_rand = np.random.uniform(0, 1, da.shape)
+
+#     # Ensure that the number of bits is valid for Round
+#     bits_params = get_bits_params(da)
+#     if "Round" or "Log" in [c.name for c in compressors]:
+#         # FIXME: We add one to the minimum??
+#         bits_min = compute_min_bits(da, bits_params) + 1
+#         bits_max = bits_params["width"]
+#         bits = range(bits_min, bits_max + 1)
+#     else:
+#         bits = bits_params["width"]
+
+#     da_sigmas = xr.DataArray(
+#         np.nan,
+#         name=da.name,
+#         dims=["compressor", "bits"],
+#         coords={
+#             "compressor": [c.name for c in compressors],
+#             "bits": bits,
+#         },
+#     )
+
+#     if (da.values.ravel() == da.values.ravel()[0]).all():
+#         print("All values are the same, skipping sigma calculation")
+#         return da_sigmas
+
+#     for compressor in da_sigmas.compressor:
+#         print(f"Compressor: {compressor.values}")
+#         for bits in tqdm(da_sigmas.bits):
+#             try:
+#                 da_decompressed = run_compressor_single(
+#                     da,
+#                     eval(f"fcpy.{compressor.values}()"),
+#                     int(bits.values),
+#                 )
+#             except:
+#                 import traceback
+
+#                 trace = traceback.format_exc()
+#                 print(f"Failed: bits={bits} compressor={compressor.values}")
+#                 print(trace)
+#                 continue
+#             da_diff = da_decompressed - da
+#             # Standardize to same range as arr_rand
+#             da_diff_normalised = compute_z_score(da_diff)
+#             sigma_ratio = estimate_sigma(
+#                 da_diff_normalised.squeeze().values
+#             ) / estimate_sigma(arr_rand)
+#             idx = {"compressor": compressor, "bits": bits}
+#             da_sigmas.loc[idx] = sigma_ratio
+#     return da_sigmas
+
+
+# def compute_knees_field(
+#     da: xr.DataArray,
+#     da_sigmas: xr.DataArray,
+#     plot=False,
+#     interp_method="polynomial",
+#     polynomial_degree=4,
+# ) -> dict:
+#     """Computes the point of maximum curvature.
+
+#     Args:
+#         da (xr.DataArray): Reference DataArray.
+#         da_sigmas (xr.DataArray): DataArray of Sigmas.
+#         plot (bool, optional): Whether to plot. Defaults to False.
+#         interp_method (str, optional): Method of interpolation to use by the knee finding algorithm. Defaults to "polynomial".
+#         polynomial_degree (int, optional): Degree of polynomial to use by the knee finding algorithm. Defaults to 4.
+
+#     Returns:
+#         dict: Dictionary of DataArrays of number of bits and sigmas.
+#     """
+#     if "lat" in da.coords:
+#         spatial_coords = ["lat", "lon"]
+#     elif "latitude" in da.coords:
+#         spatial_coords = ["latitude", "longitude"]
+#     da_bits_knee = xr.DataArray(
+#         np.nan,
+#         name="bits",
+#         coords=da.squeeze().drop(spatial_coords).coords,
+#     )
+#     assert da_bits_knee.size == 1, da_bits_knee.size
+
+#     da_bits_knee = da_bits_knee.squeeze()
+#     da_bits_knee = da_bits_knee.expand_dims(compressor=da_sigmas.compressor).copy()
+#     da_sigmas_knee = xr.full_like(da_bits_knee, np.nan).rename("sigma")
+
+#     for compressor in da_bits_knee.compressor:
+#         da_sigma = da_sigmas.sel(compressor=compressor).dropna(dim="bits")
+#         if np.isnan(da_sigma.values).all():
+#             continue
+#         kl = KneeLocator(
+#             da_sigma.bits,
+#             da_sigma.values,
+#             curve="concave",
+#             direction="increasing",
+#             online=True,
+#             interp_method=interp_method,
+#             polynomial_degree=polynomial_degree,
+#         )
+#         if kl.knee is None:
+#             continue
+#         idx = {"compressor": compressor}
+#         da_bits_knee.loc[idx] = kl.knee
+#         da_sigmas_knee.loc[idx] = da_sigma.sel(bits=kl.knee).values
+
+#         if plot:
+#             kl.plot_knee(figsize=(5, 4.5))
+#             xint = range(min(kl.x), math.ceil(max(kl.x)) + 1)
+#             plt.xticks(xint)
+#             plt.locator_params(nbins=12)
+#             plt.xlabel("Number of bits")
+#             plt.ylabel("Z test statistic score")
+#             plt.title(
+#                 f"Knee/Elbow for air specific humidity, \n"
+#                 + f"{da_bits_knee.loc[idx].coords}"
+#             )
+#             plt.show()
+
+#     return dict(bits=da_bits_knee, sigmas=da_sigmas_knee)
+
+
+# def sigmas_iterate_da(da, compressors, plot=False) -> xr.Dataset:
+#     """Computes sigmas for a given dataarray and compressors.
+
+#     Args:
+#         da (xr.DataArray): DataArray to compute sigmas for.
+#         compressors (list): List of compressors to compute sigmas for.
+
+#     Returns:
+#         xr.Dataset: Dataset with sigmas computed.
+#     """
+
+#     # First we find out all other dimensions except for the latitude and longitude as defined for a field
+#     other_dims = [
+#         dim
+#         for dim in da.dims
+#         if da[dim].attrs.get("standard_name", dim)
+#         not in ["latitude", "longitude", "values"]
+#     ]
+#     spatial_dims = {dim: 0 for dim in da.dims if dim not in other_dims}
+
+#     # Then create a dictionary with dim keys and values
+#     d_dims = {i: da[i].values for i in other_dims}
+
+#     # Create a mesh with all the combinrations that we want to iterate through
+#     mesh = [val for val in product(*d_dims.values())]
+#     # Create a mapping dim keys and values so we can use in the selection
+#     dims_mapping = []
+#     for m in mesh:
+#         d_tmp = {}
+#         for i, dim in enumerate(other_dims):
+#             d_tmp[dim] = m[i]
+#         dims_mapping.append(d_tmp)
+
+#     # Create dataarray
+#     compressors_ = [c.name for c in compressors]
+#     da_ = xr.full_like(
+#         da.isel(**spatial_dims).drop(list(spatial_dims), errors="ignore"), np.nan
+#     )
+#     da_bits = da_.expand_dims(compressor=compressors_).copy().rename("bits")
+#     da_sigma = xr.full_like(da_bits, np.nan).rename("sigma")
+#     da_bitinf = xr.full_like(da_bits, np.nan).rename("bitinf")
+
+#     # Do not use metadata from reference da
+#     da_bits.attrs = {}
+#     da_sigma.attrs = {}
+#     da_bitinf.attrs = {}
+
+#     for sel in dims_mapping:
+#         da_reference = da.sel(sel)
+#         da_sigmas = compute_sigmas(da_reference, compressors)
+#         da_knees = compute_knees_field(da_reference, da_sigmas, plot=plot)
+#         da_bits.loc[sel] = da_knees["bits"]
+#         da_sigma.loc[sel] = da_knees["sigmas"]
+#         da_bitinf.loc[sel] = fcpy.compute_required_bits_single_variable(
+#             da_reference, fcpy.get_field_chunk_fn(da_reference), [0.99]
+#         )[0][0]
+#     return xr.merge([dict(bits=da_bits, sigmas=da_sigma, bitinf=da_bitinf)])
+
+
+# def sigmas_iterate_ds(ds: xr.Dataset, compressors: list, plot=False) -> dict:
+#     """Computes sigmas for a given dataset and compressors.
+
+#     Args:
+#         ds (xr.Dataset): Dataset to compute sigmas for.
+#         compressors (list): List of compressors to compute sigmas for.
+#         plot (bool, optional): Whether to generate a plot. Defaults to False.
+
+#     Returns:
+#         dict: dictionary of Datasets with sigmas computed.
+#     """
+
+#     out = {}
+#     for var_name in ds:
+#         print(var_name)
+#         da = ds[var_name]
+#         ds_out = sigmas_iterate_da(da, compressors, plot=plot)
+#         ds_out.attrs["var_name"] = var_name
+#         ds_out.attrs["long_name"] = da.attrs["long_name"]
+#         ds_out.attrs["units"] = da.attrs["units"]
+#         out[var_name] = ds_out
+#     return out
diff --git a/packages/fcpy/fcpy/fcpy/suite.py b/packages/fcpy/fcpy/fcpy/suite.py
new file mode 100644
index 00000000..ffa10e95
--- /dev/null
+++ b/packages/fcpy/fcpy/fcpy/suite.py
@@ -0,0 +1,1002 @@
+# (C) Copyright 2022 ECMWF.
+#
+# This software is licensed under the terms of the Apache Licence Version 2.0
+# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
+# In applying this licence, ECMWF does not waive the privileges and immunities
+# granted to it by virtue of its status as an intergovernmental organisation
+# nor does it submit to any jurisdiction.
+#
+
+import itertools
+import warnings
+from collections import defaultdict
+from collections.abc import Iterator
+from time import time
+from typing import Callable, Dict, Optional, Tuple, Type, Union
+
+import cartopy.crs as ccrs
+# import fast_histogram
+# import ipywidgets as widgets
+import matplotlib.pyplot as plt
+import numpy as np
+import scipy
+import xarray as xr
+# from ipywidgets import fixed, interact
+from tqdm import tqdm
+
+from .compressors import Compressor
+# from .field import compute_bitinformation_single, compute_required_bits_for_bitinf
+from .metrics import METRICS, Metric
+from .utils import STANDARD_NAME_LAT, STANDARD_NAME_LON, get_standard_name_dims #, regrid
+
+
+def run_compressor_single(
+    da: xr.DataArray, compressor: Compressor, bits: int
+) -> xr.DataArray:
+    # output: dims=[compressor, bits, existing dims...]
+    compressed, params = compressor.compress(da.values, bits)
+    decompressed = compressor.decompress(compressed, params)
+    da2 = da.copy(data=decompressed)
+    da2 = da2.expand_dims(compressor=[compressor.name], bits=[bits])
+    return da2
+
+
+# def compute_required_bits_single_variable(
+#     da: xr.DataArray, field_fn: Callable, information_content: list[float]
+# ) -> Tuple[list[int], list[int]]:
+#     min_bits = np.full([len(information_content)], np.nan)
+#     max_bits = np.full([len(information_content)], np.nan)
+#     for field in field_fn(da):
+#         bitinf = compute_bitinformation_single(field)
+#         for i, ic in enumerate(information_content):
+#             required_bits = compute_required_bits_for_bitinf(bitinf, ic)
+#             min_bits[i] = np.nanmin([min_bits[i], required_bits])
+#             max_bits[i] = np.nanmax([max_bits[i], required_bits])
+#     return list(min_bits.astype(int)), list(max_bits.astype(int))
+
+
+# def compute_required_bits(
+#     ds: xr.Dataset, information_content: list[float]
+# ) -> Tuple[dict, dict]:
+#     out_min = {}
+#     out_max = {}
+#     for var_name in ds:
+#         da = ds[var_name]
+#         field_fn = get_field_chunk_fn(da)
+#         out_min[var_name], out_max[var_name] = compute_required_bits_single_variable(
+#             ds[var_name], field_fn, information_content
+#         )
+#     return out_min, out_max
+
+
+# def compute_required_bit_space(da: xr.DataArray, field_fn: Callable) -> list:
+#     _, bits_max = compute_required_bits_single_variable(da, field_fn, [0.99, 0.999])
+#     # FIXME should depend on dtype
+#     # FIXME sometimes bits is negative, that's why we clamp
+#     min_required_bits = max(1, bits_max[0] - 1)
+#     max_required_bits = min(32, max(1, bits_max[1]) + 1)  # - 9
+#     bits = list(range(min_required_bits, max_required_bits + 1))
+#     return bits
+
+
+def get_chunk_fn(da: xr.DataArray, chunk_dims: list[str]) -> Callable:
+    other_dims = list(set(da.dims) - set(chunk_dims))
+    other_coords = [da[dim].values for dim in other_dims]
+
+    def chunk_fn(da: xr.DataArray) -> Iterator[xr.DataArray]:
+        for sel in itertools.product(*other_coords):
+            sel_dict = dict(zip(other_dims, sel))
+            yield da.sel(**sel_dict).stack(dict(chunk=chunk_dims))
+
+    return chunk_fn
+
+
+def get_max_chunk_fn(
+    da: xr.DataArray, max_chunk_size_bytes: Optional[int] = None
+) -> Tuple[Callable, list[str]]:
+    if max_chunk_size_bytes is None:
+        max_chunk_size_bytes = 4 * 1024 * 1024 * 1024  # 4 GiB
+
+    dim_sizes = da.sizes
+
+    def get_chunk_size_bytes(dims: list[str]) -> int:
+        dtype_size = np.dtype(da.dtype).itemsize
+        return np.prod([dim_sizes[dim] for dim in dims]) * dtype_size
+
+    # Start with all dimensions and gradually remove dimensions
+    # until chunk size is within threshold.
+    chunk_dims = list(da.dims)
+
+    chunk_size_bytes = get_chunk_size_bytes(chunk_dims)
+    while chunk_size_bytes > max_chunk_size_bytes:
+        if len(chunk_dims) == 1:
+            break
+        chunk_dims.pop(0)
+        chunk_size_bytes = get_chunk_size_bytes(chunk_dims)
+
+    return get_chunk_fn(da, chunk_dims), chunk_dims
+
+
+def get_field_dims(da: xr.DataArray) -> list[str]:
+    if da.attrs.get("GRIB_gridType") == "reduced_gg":
+        assert "values" in da.dims
+        return ["values"]
+
+    standard_names = get_standard_name_dims(da)
+    if STANDARD_NAME_LON in standard_names and STANDARD_NAME_LAT in standard_names:
+        return [standard_names[STANDARD_NAME_LAT], standard_names[STANDARD_NAME_LON]]
+
+    raise RuntimeError("Could not determine horizontal dimensions of field")
+
+
+def get_field_chunk_fn(da: xr.DataArray) -> Callable:
+    chunk_dims = get_field_dims(da)
+    return get_chunk_fn(da, chunk_dims)
+
+
+# def compute_histograms_single_variable(
+#     da: xr.DataArray,
+#     baseline: Compressor,
+#     compressors: list[Compressor],
+#     metrics: list[Type[Metric]],
+#     bits: list[int],
+#     max_chunk_size_bytes: Optional[int] = None,
+#     max_chunk_fn: Optional[Callable] = None,
+# ) -> Dict[str, Tuple[xr.DataArray, xr.DataArray]]:
+#     if baseline.bits is None:
+#         raise RuntimeError("bits parameter missing in baseline")
+
+#     # Error metrics can run over larger chunks as well,
+#     # therefore those two functions are separate.
+#     if max_chunk_fn is None:
+#         max_chunk_fn, max_chunk_dims = get_max_chunk_fn(da, max_chunk_size_bytes)
+#         if set(max_chunk_dims) != set(da.dims):
+#             warnings.warn(f"{da.name}: chunk dims adjusted to {max_chunk_dims}")
+#         other_dims = set(da.dims) - set(max_chunk_dims)
+#         chunk_count = np.prod([da.sizes[dim] for dim in other_dims])
+#     else:
+#         # unknown
+#         chunk_count = None
+
+#     histogram_bins = 100
+
+#     src_histogram_freqs = xr.DataArray(
+#         np.nan,
+#         name=da.name,
+#         attrs=da.attrs,
+#         dims=["bin"],
+#         coords={
+#             "bin": np.arange(histogram_bins),
+#         },
+#     )
+
+#     src_histogram_edges = xr.DataArray(
+#         np.nan,
+#         name=da.name,
+#         attrs=da.attrs,
+#         dims=["bin_edge"],
+#         coords={
+#             "bin_edge": np.arange(histogram_bins + 1),
+#         },
+#     )
+
+#     baseline_histogram_freqs = src_histogram_freqs.copy()
+#     baseline_histogram_edges = src_histogram_edges.copy()
+
+#     decompressed_histogram_freqs = xr.DataArray(
+#         np.nan,
+#         name=da.name,
+#         attrs=da.attrs,
+#         dims=["compressor", "bits", "bin"],
+#         coords={
+#             "compressor": [c.name for c in compressors],
+#             "bits": bits,
+#             "bin": np.arange(histogram_bins),
+#         },
+#     )
+
+#     decompressed_histogram_edges = xr.DataArray(
+#         np.nan,
+#         name=da.name,
+#         attrs=da.attrs,
+#         dims=["compressor", "bits", "bin_edge"],
+#         coords={
+#             "compressor": [c.name for c in compressors],
+#             "bits": bits,
+#             "bin_edge": np.arange(histogram_bins + 1),
+#         },
+#     )
+
+#     metric_histogram_freqs = xr.DataArray(
+#         np.nan,
+#         name=da.name,
+#         attrs=da.attrs,
+#         dims=["compressor", "bits", "metric", "bin"],
+#         coords={
+#             "compressor": [c.name for c in compressors],
+#             "bits": bits,
+#             "metric": [m.name for m in metrics],
+#             "bin": np.arange(histogram_bins),
+#         },
+#     )
+
+#     metric_histogram_edges = xr.DataArray(
+#         np.nan,
+#         name=da.name,
+#         attrs=da.attrs,
+#         dims=["compressor", "bits", "metric", "bin_edge"],
+#         coords={
+#             "compressor": [c.name for c in compressors],
+#             "bits": bits,
+#             "metric": [m.name for m in metrics],
+#             "bin_edge": np.arange(histogram_bins + 1),
+#         },
+#     )
+
+#     t0 = time()
+#     for chunk in tqdm(max_chunk_fn(da), total=chunk_count):
+#         update_histogram(
+#             chunk.values,
+#             src_histogram_freqs,
+#             src_histogram_edges,
+#             log_prefix=f"{da.name} [histogram for source]: ",
+#         )
+
+#         da_baseline = run_compressor_single(chunk, baseline, baseline.bits)
+#         da_baseline = da_baseline.squeeze(dim=["compressor", "bits"])
+
+#         update_histogram(
+#             da_baseline.values,
+#             baseline_histogram_freqs,
+#             baseline_histogram_edges,
+#             log_prefix=f"{da.name} [histogram for baseline]: ",
+#         )
+
+#         for compressor in compressors:
+#             for bits_ in bits:
+#                 da_decompressed = run_compressor_single(chunk, compressor, bits_)
+
+#                 idx = dict(
+#                     compressor=compressor.name,
+#                     bits=bits_,
+#                 )
+
+#                 freqs = decompressed_histogram_freqs.sel(idx)
+#                 edges = decompressed_histogram_edges.sel(idx)
+#                 update_histogram(
+#                     da_decompressed.values,
+#                     freqs,
+#                     edges,
+#                     log_prefix=f"{da.name} [histogram for decompressed: {compressor.name} @ {bits_} bits]: ",
+#                 )
+
+#                 for metric in metrics:
+#                     out = metric().compute(da_baseline.values, da_decompressed.values)
+
+#                     idx = dict(
+#                         compressor=compressor.name,
+#                         bits=bits_,
+#                         metric=metric.name,
+#                     )
+
+#                     freqs = metric_histogram_freqs.sel(idx)
+#                     edges = metric_histogram_edges.sel(idx)
+#                     update_histogram(
+#                         out,
+#                         freqs,
+#                         edges,
+#                         log_prefix=f"{da.name} [histogram for metric: {metric.name} for {compressor.name} @ {bits_} bits]: ",
+#                     )
+
+#     print(f"{time()-t0} s")
+
+#     return {
+#         "source": (src_histogram_freqs, src_histogram_edges),
+#         "baseline": (baseline_histogram_freqs, baseline_histogram_edges),
+#         "decompressed": (decompressed_histogram_freqs, decompressed_histogram_edges),
+#         "metric": (metric_histogram_freqs, metric_histogram_edges),
+#     }
+
+
+# def update_histogram(
+#     data: np.ndarray, freqs: np.ndarray, edges: np.ndarray, log_prefix=""
+# ):
+#     data = data.ravel()
+
+#     # mask infinity with nan to avoid min/max picking it up below
+#     data = data[np.isfinite(data)]
+#     if data.size == 0:
+#         print(f"{log_prefix}ignoring, data is all-nan/inf")
+#         return
+
+#     bins = len(edges) - 1
+#     is_first = np.isnan(edges[0].item())
+#     if is_first:
+#         hist_min, hist_max = (np.min(data), np.max(data))
+
+#         if hist_min == hist_max:
+#             old_hist_min = hist_min
+#             eps = np.finfo(data.dtype).eps
+#             hist_min = hist_min - eps * bins
+#             hist_max = hist_max + eps * bins
+#             print(
+#                 f"{log_prefix}"
+#                 f"metric values min/max are identical ({old_hist_min}), "
+#                 f"using [{hist_min}, {hist_max}] as histogram range"
+#             )
+#         edges[:] = np.linspace(hist_min, hist_max, bins + 1)
+#         hist_range = edges[0], edges[-1]
+#         freqs[:] = fast_histogram.histogram1d(data, bins=bins, range=hist_range)
+#     else:
+#         hist_min, hist_max = edges[0].item(), edges[-1].item()
+
+#         # check if min/max of all following chunks is roughly
+#         # the same as the initial min/max, else print a warning
+#         current_min, current_max = (np.min(data), np.max(data))
+#         eps_factor = 0.01
+#         eps = np.finfo(data.dtype).eps
+#         eps_min = max(eps, abs(eps_factor * hist_min))
+#         eps_max = max(eps, abs(eps_factor * hist_max))
+#         if current_min < hist_min - eps_min or current_max > hist_max + eps_max:
+#             print(
+#                 f"{log_prefix}"
+#                 f"min/max of chunk is outside range by >{eps_factor*100}%, histogram may be off "
+#                 f"(initial: [{hist_min}, {hist_max}], current: [{current_min}, {current_max}])"
+#             )
+
+#         if hist_min != hist_max:
+#             hist_range = (hist_min, hist_max)
+#             freqs += fast_histogram.histogram1d(data, bins=bins, range=hist_range)
+
+
+# def compute_histograms(
+#     ds: xr.Dataset,
+#     baseline: Compressor,
+#     compressors: list[Compressor],
+#     metrics: list[Type[Metric]],
+#     bits: dict[str, list[int]] = None,
+#     max_chunk_size_bytes: Optional[int] = None,
+# ) -> Dict[str, Tuple[xr.Dataset, xr.Dataset]]:
+#     histogram_frequencies = defaultdict(list)
+#     histogram_edges = defaultdict(list)
+#     for var_name in ds:
+#         da = ds[var_name]
+#         histograms = compute_histograms_single_variable(
+#             da,
+#             baseline,
+#             compressors,
+#             metrics,
+#             bits=bits[var_name],
+#             max_chunk_size_bytes=max_chunk_size_bytes,
+#         )
+#         for hist_type, (freqs, edges) in histograms.items():
+#             histogram_frequencies[hist_type].append(freqs)
+#             histogram_edges[hist_type].append(edges)
+#     out = {}
+#     for hist_type in histogram_frequencies.keys():
+#         out[hist_type] = (
+#             xr.merge(histogram_frequencies[hist_type], combine_attrs="drop"),
+#             xr.merge(histogram_edges[hist_type], combine_attrs="drop"),
+#         )
+#     return out
+
+
+def compute_stats(
+    freqs: xr.Dataset,
+    bins: xr.Dataset,
+    var_names: Optional[list[str]] = None,
+    compressors: Optional[list[str]] = None,
+    metrics: Optional[list[str]] = None,
+    bits: Optional[list[str]] = None,
+) -> xr.Dataset:
+    reductions = {
+        "median": lambda dist: dist.median(),
+        "mean": lambda dist: dist.mean(),
+        "std": lambda dist: dist.std(),
+        "var": lambda dist: dist.var(),
+        "q1": lambda dist: dist.ppf([0.25])[0],
+        "q3": lambda dist: dist.ppf([0.75])[0],
+        # Compute the minimum and maximum over the edges of the histogram
+        "min": lambda dist: min(dist.ppf([0])[0], dist.ppf([1])[0]),
+        "max": lambda dist: max(dist.ppf([0])[0], dist.ppf([1])[0]),
+    }
+    if var_names is None:
+        var_names = list(freqs)
+    if compressors is None:
+        compressors = freqs.compressor.values
+    if metrics is None:
+        metrics = freqs.metric.values
+    if bits is None:
+        bits = freqs.bits.values
+
+    arr = []
+    for var_name in var_names:
+        stats = xr.DataArray(
+            np.nan,
+            name=var_name,
+            dims=["compressor", "bits", "metric", "reduction"],
+            coords={
+                "compressor": compressors,
+                "bits": bits,
+                "metric": metrics,
+                "reduction": list(reductions.keys()),
+            },
+        )
+        for compressor in compressors:
+            for bits_ in bits:
+                for metric in metrics:
+                    idx = {"compressor": compressor, "bits": bits_, "metric": metric}
+                    hist = (freqs[var_name].loc[idx], bins[var_name].loc[idx])
+                    hist_dist = scipy.stats.rv_histogram(hist)
+                    stats.loc[idx] = [fn(hist_dist) for fn in reductions.values()]
+
+        arr.append(stats)
+
+    out = xr.merge(arr)
+    return out
+
+
+def compute_decompressed_stats(
+    reference_freqs: xr.Dataset,
+    reference_bins: xr.Dataset,
+    decompressed_freqs: xr.Dataset,
+    decompressed_bins: xr.Dataset,
+    var_names: Optional[list[str]] = None,
+    compressors: Optional[list[str]] = None,
+    bits: Optional[list[str]] = None,
+) -> xr.Dataset:
+    reductions = {
+        "snr": lambda reference_dist, decompressed_dist: reference_dist.var()
+        / decompressed_dist.var(),
+    }
+    if var_names is None:
+        var_names = list(decompressed_freqs)
+    if compressors is None:
+        compressors = decompressed_freqs.compressor.values
+    if bits is None:
+        bits = decompressed_freqs.bits.values
+
+    arr = []
+    for var_name in var_names:
+        stats = xr.DataArray(
+            np.nan,
+            name=var_name,
+            dims=["compressor", "bits", "reduction"],
+            coords={
+                "compressor": compressors,
+                "bits": bits,
+                "reduction": list(reductions.keys()),
+            },
+        )
+
+        baseline_hist = (reference_freqs[var_name], reference_bins[var_name])
+        baseline_hist_dist = scipy.stats.rv_histogram(baseline_hist)
+
+        for compressor in compressors:
+            for bits_ in bits:
+                idx = {"compressor": compressor, "bits": bits_}
+                decompressed_hist = (
+                    decompressed_freqs[var_name].loc[idx],
+                    decompressed_bins[var_name].loc[idx],
+                )
+                decompressed_hist_dist = scipy.stats.rv_histogram(decompressed_hist)
+                stats.loc[idx] = [
+                    fn(baseline_hist_dist, decompressed_hist_dist)
+                    for fn in reductions.values()
+                ]
+
+        arr.append(stats)
+
+    out = xr.merge(arr)
+    return out
+
+
+def compute_stats_direct(
+    ds: xr.Dataset,
+    baseline: Compressor,
+    compressors: list[Compressor],
+    metrics: list[Type[Metric]],
+    bits: dict[str, list[int]],
+):
+    das = []
+    for var_name in ds:
+        da = ds[var_name]
+        das.append(
+            compute_stats_direct_single_variable(
+                da, baseline, compressors, metrics, bits[var_name]
+            )
+        )
+    return xr.merge(das)
+
+
+def compute_stats_direct_single_variable(
+    da: xr.DataArray,
+    baseline: Compressor,
+    compressors: list[Compressor],
+    metrics: list[Type[Metric]],
+    bits: list[str],
+):
+    reductions = {
+        "median": lambda d: np.median(d),
+        "mean": lambda d: d.mean(),
+        "std": lambda d: d.std(),
+        "var": lambda d: d.var(),
+        "q1": lambda d: np.quantile(d, 0.25),
+        "q3": lambda d: np.quantile(d, 0.75),
+        "min": lambda d: d.min(),
+        "max": lambda d: d.max(),
+    }
+    stats = xr.DataArray(
+        np.nan,
+        name=da.name,
+        dims=["compressor", "bits", "metric", "reduction"],
+        coords={
+            "compressor": [c.name for c in compressors],
+            "bits": bits,
+            "metric": [m.name for m in metrics],
+            "reduction": list(reductions.keys()),
+        },
+    )
+
+    da_baseline = run_compressor_single(da, baseline, baseline.bits)
+    da_baseline = da_baseline.squeeze(dim=["compressor", "bits"])
+
+    for compressor in compressors:
+        for bits_ in bits:
+            da_decompressed = run_compressor_single(da, compressor, bits_)
+
+            for metric in metrics:
+                out = metric().compute(da_baseline.values, da_decompressed.values)
+
+                for reduction, fn in reductions.items():
+                    idx = dict(
+                        compressor=compressor.name,
+                        bits=bits_,
+                        metric=metric.name,
+                        reduction=reduction,
+                    )
+                    stats.loc[idx] = fn(out)
+    return stats
+
+
+def compute_custom_metrics(
+    ds: xr.Dataset,
+    baseline: Compressor,
+    compressors: list[Compressor],
+    custom_metrics: list[Callable],
+    bits: dict[str, list[int]],
+    max_chunk_size_bytes: Optional[int],
+) -> list[xr.Dataset]:
+    das_metric = defaultdict(list)
+    for var_name in ds:
+        da = ds[var_name]
+        bits_ = bits[var_name]
+        max_chunk_fn, max_chunk_dims = get_max_chunk_fn(da, max_chunk_size_bytes)
+        for i, fn in enumerate(custom_metrics):
+            chunks = max_chunk_fn(da)
+            da_metric = fn(chunks, baseline, compressors, bits_)
+            da_metric.name = var_name
+            das_metric[i].append(da_metric)
+    out = [xr.merge(das_metric[i]) for i in das_metric]
+    return out
+
+
+# class Suite:
+#     """Create a suite.
+
+#     Args:
+#         ds (xr.Dataset): The dataset to use.
+#         baseline (Compressor): The compressor to use for the baseline. Must include bits in the constructor.
+#         compressors (list[Compressor]): The list of compressors to use.
+#         metrics (list[Type[Metric]]): The list of metrics to compute.
+#         custom_metrics (list[Callable], optional): Custom metrics as user functions.
+#             Each function is called as fn(chunks, baseline, compressors, bits) and
+#             must return an xarray DataArray.
+#         bits (list[int], optional): List of bits values to iterate over. Defaults to Klwer et al. (2021)'s bit-information metric.
+#         max_chunk_size_bytes (int, optional): Maximum size in bytes that a chunk may have. Defaults to 4 GiB.
+#         skip_histograms (bool, optional): If True, compute metrics directly instead of using histograms.
+#     """
+
+#     def __init__(
+#         self,
+#         ds: xr.Dataset,
+#         baseline: Compressor,
+#         compressors: list[Compressor],
+#         metrics: list[Type[Metric]],
+#         custom_metrics: Optional[list[Callable]] = None,
+#         bits: Optional[Union[dict, list[int]]] = None,
+#         max_chunk_size_bytes: Optional[int] = None,
+#         skip_histograms=False,
+#     ):
+#         if bits is None:
+#             bits_per_var = {}
+#             for var_name in ds:
+#                 # Bitinformation always needs to run over fields.
+#                 field_chunk_fn = get_field_chunk_fn(ds[var_name])
+#                 bits_ = compute_required_bit_space(ds[var_name], field_chunk_fn)
+#                 bits_per_var[var_name] = bits_
+#                 print(f"{var_name}: bits not given, computed as {bits_}")
+#         elif isinstance(bits, list):
+#             bits_per_var = {var_name: bits for var_name in ds}
+#         elif isinstance(bits, dict):
+#             bits_per_var = bits
+#         else:
+#             raise ValueError("bits must be a list or dict")
+
+#         self.ds = ds
+#         self.baseline = baseline
+#         self.metrics = metrics
+#         self.bits = bits_per_var
+#         if skip_histograms:
+#             self.histograms = None
+#             self.stats = compute_stats_direct(
+#                 ds,
+#                 baseline,
+#                 compressors,
+#                 metrics,
+#                 bits=bits_per_var,
+#             )
+#         else:
+#             self.stats = None
+#             self.histograms = compute_histograms(
+#                 ds,
+#                 baseline,
+#                 compressors,
+#                 metrics,
+#                 bits=bits_per_var,
+#                 max_chunk_size_bytes=max_chunk_size_bytes,
+#             )
+#         self.bits_min, self.bits_max = compute_required_bits(ds, [0.99, 0.999])
+
+#         if custom_metrics is None:
+#             self.custom_metrics = []
+#         else:
+#             self.custom_metrics = compute_custom_metrics(
+#                 ds,
+#                 baseline,
+#                 compressors,
+#                 custom_metrics,
+#                 bits=bits_per_var,
+#                 max_chunk_size_bytes=max_chunk_size_bytes,
+#             )
+
+#     def snr(self, reference: str = "baseline"):
+#         # TODO doesn't work without histograms yet
+#         assert reference in ["source", "baseline"]
+#         freqs_reference, edges_reference = self.histograms[reference]
+#         freqs_decompressed, edges_decompressed = self.histograms["decompressed"]
+
+#         stats = compute_decompressed_stats(
+#             freqs_reference, edges_reference, freqs_decompressed, edges_decompressed
+#         )
+#         return stats.sel(reduction="snr")
+
+#     def compute_metric_stats(
+#         self,
+#         var_names: Optional[list[str]] = None,
+#         metrics: Optional[list[str]] = None,
+#         compressors: Optional[list[str]] = None,
+#         bits: Optional[list[int]] = None,
+#     ):
+#         if self.histograms:
+#             freqs, edges = self.histograms["metric"]
+
+#             stats = compute_stats(
+#                 freqs,
+#                 edges,
+#                 var_names=var_names,
+#                 metrics=metrics,
+#                 compressors=compressors,
+#                 bits=bits,
+#             )
+#         else:
+#             stats = self.stats
+#             if var_names is not None:
+#                 stats = stats[var_names]
+#             if metrics is not None:
+#                 stats = stats.sel(metric=metrics)
+#             if compressors is not None:
+#                 stats = stats.sel(compressor=compressors)
+#             if bits is not None:
+#                 stats = stats.sel(bits=bits)
+
+#         return stats
+
+#     def lineplot(
+#         self,
+#         metric: Type[Metric],
+#         reduction: str,
+#         var_names: Optional[list[str]] = None,
+#     ):
+#         if var_names is None:
+#             var_names = list(self.ds)
+#         else:
+#             var_names = list(var_names)
+#         if len(var_names) == 0:
+#             return
+
+#         stats = self.compute_metric_stats(
+#             var_names=var_names,
+#             metrics=[metric.name],
+#         )
+
+#         # TODO: add choose colorscheme
+#         from cycler import cycler
+
+#         colorlist = ["#648FFF", "#785EF0", "#DC267F", "k"]
+#         custom_cycler = cycler(color=colorlist)
+
+#         for var_name in var_names:
+#             fig, ax = plt.subplots()
+#             ax.set_prop_cycle(custom_cycler)
+#             vals = stats[var_name].sel(metric=metric.name, reduction=reduction)
+#             vals.plot.line(x="bits", hue="compressor", ax=ax)
+#             new_list = range(
+#                 int(np.floor(vals.bits.min())), int(np.ceil(vals.bits.max())) + 1
+#             )
+#             ax.set_xticks(new_list)
+#             ax.set_xlabel("Number of bits")
+#             # TODO: fix units
+#             ax.set_ylabel(
+#                 f"{self.ds[var_name].long_name} {metric.name} in {self.ds[var_name].units}"
+#             )
+
+#             text_offset = 0.05
+#             ax.axvline(x=16, color="k", linewidth=2)
+#             ax.text(16 + text_offset, vals.min(), "Current @ 16 bits", rotation=90)
+#             bits_99 = self.bits_max[var_name][0]
+#             bits_100 = self.bits_max[var_name][1]
+#             ax.axvline(x=bits_99, color="k", linewidth=2)
+#             ax.text(
+#                 bits_99 + text_offset, vals.min(), "BitInformation @ 99 %", rotation=90
+#             )
+#             ax.axvline(x=bits_100, color="k", linewidth=2)
+#             ax.text(
+#                 bits_100 + text_offset,
+#                 vals.min(),
+#                 "BitInformation @ 100 %",
+#                 rotation=90,
+#             )
+
+#             fig.show()
+
+#     def histplot(
+#         self,
+#         metric: Type[Metric],
+#         compressor: Compressor,
+#         bits: int,
+#         var_names: Optional[list[str]] = None,
+#     ):
+#         freqs, edges = self.histograms["metric"]
+#         freqs_sel = freqs.sel(metric=metric.name, compressor=compressor, bits=bits)
+#         bins_sel = edges.sel(metric=metric.name, compressor=compressor, bits=bits)
+
+#         if var_names is None:
+#             var_names = list(self.ds)
+#         if len(var_names) == 0:
+#             return
+#         for var_name in var_names:
+#             _, ax = plt.subplots()
+#             plt.stairs(freqs_sel[var_name].values, bins_sel[var_name].values, fill=True)
+#             # TODO: fix units
+#             ax.set_xlabel(
+#                 f"{self.ds[var_name].long_name} {metric.name} in {self.ds[var_name].units}"
+#             )
+#             ax.set_ylabel("Frequency")
+#             ax.set_title(f"Compressor: {compressor}, Bits: {bits}")
+#             plt.show()
+
+#     def boxplot(
+#         self,
+#         metric: Type[Metric],
+#         compressor: str,
+#         bits: int,
+#         var_names: Optional[list[str]] = None,
+#     ):
+#         stats = self.compute_metric_stats(
+#             compressors=[compressor],
+#             metrics=[metric.name],
+#             bits=[bits],
+#         )
+
+#         if var_names is None:
+#             var_names = list(self.ds)
+#         if len(var_names) == 0:
+#             return
+#         for var_name in var_names:
+#             s = stats[var_name].squeeze()
+#             bxpstats = dict(
+#                 med=s.sel(reduction="median").item(),
+#                 q1=s.sel(reduction="q1").item(),
+#                 q3=s.sel(reduction="q3").item(),
+#                 whislo=s.sel(reduction="min").item(),
+#                 whishi=s.sel(reduction="max").item(),
+#                 label=f"{self.ds[var_name].long_name}",
+#             )
+
+#             _, ax = plt.subplots()
+#             ax.bxp([bxpstats], showfliers=False)
+#             # TODO: fix units
+#             ax.set_ylabel(f"{metric.name.capitalize()} in {self.ds[var_name].units}")
+#             ax.set_title(f"Compressor: {compressor}, Bits: {bits}")
+
+#     def nblineplot(self):
+#         metric = [(m.name, m) for m in self.metrics]
+#         reduction = ["max", "min", "mean", "std"]
+#         var_names_all = list(self.ds.keys())
+#         var_names = widgets.SelectMultiple(
+#             options=var_names_all, value=[var_names_all[0]], description="Variables"
+#         )
+#         interact(self.lineplot, metric=metric, reduction=reduction, var_names=var_names)
+
+
+def plot_spatial_single(da, ds, var_name, metric):
+    fig, ax = plt.subplots(1, 1, subplot_kw={"projection": ccrs.EqualEarth()})
+    regridded = da.squeeze()
+    im = regridded.plot.imshow(
+        ax=ax, transform=ccrs.PlateCarree(), levels=10, add_colorbar=False
+    )
+    ax.set_title(f"")
+    ax.coastlines()
+    cbar_rect_left = ax.get_position().x1 + 0.02
+    cbar_rect_bottom = ax.get_position().y0
+    cbar_rect_width = 0.02
+    cbar_rect_height = ax.get_position().height
+    cax = fig.add_axes(
+        [cbar_rect_left, cbar_rect_bottom, cbar_rect_width, cbar_rect_height]
+    )
+    plt.colorbar(im, cax=cax, label=f"{ds[var_name].long_name} in {ds[var_name].units}")
+    ax.set_title(f"{metric.capitalize()}")
+    plt.show()
+
+
+# def spatialplot(
+#     ds: xr.Dataset,
+#     baseline: Compressor,
+#     var_name: str,
+#     compressor: Compressor,
+#     metric: Type[Metric],
+#     latitude: float,
+#     longitude: float,
+#     third_dim: Optional[str] = None,
+#     **sel: dict,
+# ):
+#     standard_names = get_standard_name_dims(ds)
+#     is_gridded = (
+#         STANDARD_NAME_LAT in standard_names and STANDARD_NAME_LON in standard_names
+#     )
+
+#     plot_sel = {}
+#     if third_dim and third_dim in sel:
+#         plot_sel[third_dim] = sel.pop(third_dim)
+
+#     da_sel = ds[var_name].sel(sel)
+
+#     assert baseline.bits is not None
+#     assert compressor.bits is not None
+#     da_baseline = run_compressor_single(da_sel, baseline, baseline.bits)
+#     da_baseline = da_baseline.squeeze(dim=["compressor", "bits"])
+#     da_decompressed = run_compressor_single(da_sel, compressor, compressor.bits)
+
+#     da = metric().compute(da_baseline, da_decompressed)
+#     da.attrs = {
+#         "long_name": da_sel.attrs["long_name"],
+#         "units": da_sel.attrs["units"],
+#     }
+
+#     if third_dim not in da.dims:
+#         third_dim = None
+
+#     das = [da, da_baseline, da_decompressed]
+#     das_regridded = []
+#     for da_ in das:
+#         if is_gridded:
+#             das_regridded.append(da_)
+#         else:
+#             out = []
+#             path_to_template = ds.attrs.get("path")
+#             if path_to_template is None:
+#                 raise RuntimeError(
+#                     "Cannot regrid, 'path' attribute missing in xarray Dataset"
+#                 )
+#             if third_dim:
+#                 for third_dim_val in list(da_[third_dim].values):
+#                     regridded = regrid(
+#                         source=da_.sel({third_dim: third_dim_val}),
+#                         path_to_template=path_to_template,
+#                     )
+#                     regridded = regridded.assign_coords({third_dim: [third_dim_val]})
+#                     out.append(regridded)
+#             else:
+#                 regridded = regrid(source=da_, path_to_template=path_to_template)
+#                 out.append(regridded)
+#             regridded = xr.merge(out)[var_name]
+#             das_regridded.append(regridded)
+#             standard_names = get_standard_name_dims(regridded)
+
+#     da, da_baseline, da_decompressed = das_regridded
+
+#     plot_spatial_single(
+#         da_baseline.sel({third_dim: plot_sel[third_dim]}),
+#         ds,
+#         var_name,
+#         metric="Baseline",
+#     )
+#     plot_spatial_single(
+#         da_decompressed.sel({third_dim: plot_sel[third_dim]}),
+#         ds,
+#         var_name,
+#         metric="Decompressed",
+#     )
+#     plot_spatial_single(
+#         da.sel({third_dim: plot_sel[third_dim]}),
+#         ds,
+#         var_name,
+#         metric=metric.name,
+#     )
+
+#     if third_dim:
+#         fig, ax = plt.subplots(
+#             nrows=1, ncols=3, figsize=(25, 5), width_ratios=[2, 2, 1]
+#         )
+#         # First column: lat/third dim slice
+#         im = da.sel({standard_names["longitude"]: longitude}, method="nearest").plot(
+#             ax=ax[0], add_colorbar=False
+#         )
+#         plt.colorbar(im, ax=ax[0]).set_label(
+#             label=f"{ds[var_name].long_name} {metric.name} in {ds[var_name].units}"
+#         )
+#         # Second column: lon/third dim slice
+#         im = da.sel({standard_names["latitude"]: latitude}, method="nearest").plot(
+#             ax=ax[1], add_colorbar=False
+#         )
+#         plt.colorbar(im, ax=ax[1]).set_label(
+#             label=f"{ds[var_name].long_name} {metric.name} in {ds[var_name].units}"
+#         )
+#         # Third column: third dim profile
+#         im = da.sel(
+#             {
+#                 standard_names["longitude"]: longitude,
+#                 standard_names["latitude"]: latitude,
+#             },
+#             method="nearest",
+#         ).plot(y=third_dim, ax=ax[2])
+#         ax[2].set_xlabel(
+#             f"{ds[var_name].long_name} {metric.name} in {ds[var_name].units}"
+#         )
+#         fig.tight_layout()
+#     plt.show()
+
+
+# def nbspatialplot(
+#     ds: xr.Dataset, baseline: Compressor, compressor: Compressor, third_dim=None
+# ):
+#     var_name = list(ds)
+#     metric = [(m.name, m) for m in METRICS]
+
+#     standard_names = get_standard_name_dims(ds)
+#     dims_to_exclude = set(["values"])
+
+#     for name in [STANDARD_NAME_LAT, STANDARD_NAME_LON]:
+#         if name in standard_names:
+#             dims_to_exclude.add(standard_names[name])
+
+#     dims = set(ds.dims) - dims_to_exclude
+#     sel = {dim: ds[dim].values for dim in dims if ds.dims[dim] > 1}
+#     if third_dim:
+#         sel[third_dim] = widgets.SelectionSlider(options=ds[third_dim].values)
+#     latitude = widgets.IntSlider(min=-90, max=90, step=1, value=0)
+#     longitude = widgets.IntSlider(min=-180, max=180, step=1, value=0)
+
+#     interact(
+#         spatialplot,
+#         ds=fixed(ds),
+#         baseline=fixed(baseline),
+#         compressor=fixed(compressor),
+#         metric=metric,
+#         var_name=var_name,
+#         latitude=latitude,
+#         longitude=longitude,
+#         third_dim=fixed(third_dim),
+#         **sel,
+#     )
diff --git a/packages/fcpy/fcpy/fcpy/utils.py b/packages/fcpy/fcpy/fcpy/utils.py
new file mode 100644
index 00000000..72b29dc8
--- /dev/null
+++ b/packages/fcpy/fcpy/fcpy/utils.py
@@ -0,0 +1,162 @@
+# (C) Copyright 2022 ECMWF.
+#
+# This software is licensed under the terms of the Apache Licence Version 2.0
+# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
+# In applying this licence, ECMWF does not waive the privileges and immunities
+# granted to it by virtue of its status as an intergovernmental organisation
+# nor does it submit to any jurisdiction.
+#
+
+import tempfile
+from typing import Optional
+
+# import climetlab as cml
+import eccodes as ecc
+import numpy as np
+import xarray as xr
+
+
+# def regrid(
+#     source: xr.DataArray,
+#     path_to_template: str,
+#     dx: float = 1,
+#     dy: float = 1,
+#     new_labels: Optional[dict] = None,
+# ) -> xr.DataArray:
+#     """Regrid a Gaussian grid field into a regular latitude longitude grid.
+
+#     Args:
+#         source (xr.DataArray): field in Gaussian grid
+#         path_to_template (str): path to GRIB in Gaussian grid
+#         dx (int, optional): longitude grid resolution in arc-degree. Defaults to 1.
+#         dy (int, optional): latitude grid resolution in arc-degree. Defaults to 1.
+#         new_labels (Optional[dict], optional): Name of regridded xr.DataArray. Defaults to None.
+
+#     Returns:
+#         xr.DataArray: Regular latitude longitude grid
+#     """
+
+#     try:
+#         import metview as mv
+#     except:
+#         raise ImportError("Metview could not be imported.")
+
+#     assert isinstance(source, xr.DataArray), "source must be of type xarray.DataArray"
+
+#     with open(path_to_template, "rb") as file_template:
+#         with tempfile.NamedTemporaryFile("wb") as file_filled:
+#             # Read the first message in the file
+#             msgid = ecc.codes_grib_new_from_file(file_template)
+
+#             # Get data/metadata
+#             paramId = ecc.codes_get(msgid, "paramId")
+
+#             # Set data/metadata
+#             # TODO: set missing value
+#             # MISSING_VALUE = 9998
+#             # ecc.codes_set(msgid, 'missingValue', MISSING_VALUE)
+
+#             # overwrite the paramId with an arbitrary experimental paramId
+#             # to avoid issues with interpolation in case of special paramId
+#             ecc.codes_set(msgid, "paramId", 80)
+
+#             ecc.codes_set_values(msgid, source.values.flatten())
+
+#             ecc.codes_write(msgid, file_filled)
+
+#             ecc.codes_release(msgid)
+
+#             # Regrid data
+#             target_grid = {"grid": [dx, dy], "interpolation": "nearest_neighbour"}
+
+#             data = mv.read(file_filled.name)
+#             with tempfile.TemporaryDirectory() as tmpdirname:
+#                 path_to_regridded = f"{tmpdirname}/file_regridded.grib"
+
+#                 _ = mv.regrid(target_grid, data=data, target=path_to_regridded)
+
+#                 ds_regridded = cml.load_source("file", path_to_regridded).to_xarray()
+#                 assert len(list(ds_regridded)) == 1
+#                 da_regridded = ds_regridded[list(ds_regridded)[0]]
+
+#         da_regridded.attrs = {
+#             "long_name": source.attrs["long_name"],
+#             "units": source.attrs["units"],
+#         }
+#         da_regridded.name = source.name
+
+#     return da_regridded
+
+
+STANDARD_NAME_LAT = "latitude"
+STANDARD_NAME_LON = "longitude"
+
+
+def get_standard_name_dims(ds: xr.Dataset) -> dict:
+    """Return the standard_name of each dimention in the dataset
+
+    Args:
+        ds (xr.Dataset): _description_
+
+    Returns:
+        dict: _description_
+    """
+    return {
+        c.standard_name: c.name
+        for c in [ds[dim] for dim in ds.dims]
+        if "standard_name" in c.attrs
+    }
+
+
+def compute_z_score(da: xr.DataArray) -> xr.DataArray:
+    """Normalize array to [0,1]"""
+    return (da - da.min()) / (da.max() - da.min())
+
+
+def get_bits_params(da: xr.DataArray) -> dict:
+    if da.dtype == np.float32:
+        dtype_int = np.uint32
+        width = 32
+        sign_and_exponent_bits = 9
+    elif da.dtype == np.float64:
+        dtype_int = np.uint64
+        width = 64
+        sign_and_exponent_bits = 12
+    else:
+        raise RuntimeError("unsupported dtype")
+    return dict(
+        dtype_int=dtype_int, width=width, sign_and_exponent_bits=sign_and_exponent_bits
+    )
+
+
+def to_bits(da, bits_params):
+    value_range = np.linspace(da.min(), da.max(), 10000).astype(da.dtype)
+    l = []
+    for i in value_range:
+        l.append(
+            np.array(
+                list(
+                    np.binary_repr(
+                        int(i.view(dtype=bits_params["dtype_int"])),
+                        width=bits_params["width"],
+                    )
+                ),
+                dtype=bits_params["dtype_int"],
+            )
+        )
+    l = np.vstack(l)
+    l2 = []
+    for i in range(l.shape[1]):
+        l2.append(l[:, i])
+
+    return np.array(l2)
+
+
+def compute_min_bits(da, bits_params):
+    bits_arr = to_bits(da, bits_params)
+    used_sign_and_exponent_bits = 0
+    for col in range(bits_params["sign_and_exponent_bits"]):
+        if all(bits_arr[col, :] == 0) or all(bits_arr[col, :] == 1):
+            continue
+        used_sign_and_exponent_bits += 1
+    return used_sign_and_exponent_bits
diff --git a/packages/fcpy/fcpy/pyproject.toml b/packages/fcpy/fcpy/pyproject.toml
new file mode 100644
index 00000000..4766a0cd
--- /dev/null
+++ b/packages/fcpy/fcpy/pyproject.toml
@@ -0,0 +1,36 @@
+[project]
+name = "fcpy"
+version = "0.2.0"
+authors = [
+    { name = "ECMWF" },
+    { name = "Juniper Tyree", email = "juniper.tyree@helsinki.fi" },
+]
+description = "ECMWF Field Compression Laboratory"
+license = { file = "LICENSE.txt" }
+requires-python = ">=3.8"
+classifiers = [
+    "Development Status :: 3 - Alpha",
+    "Intended Audience :: Science/Research",
+    "Programming Language :: Python :: 3",
+]
+dependencies = [
+    "numpy",
+    "scipy",
+    "xarray",
+    "dask",
+    "matplotlib",
+    "cartopy",
+    "scikit-learn",
+    "scikit-image",
+]
+
+[project.urls]
+homepage = "https://github.com/ecmwf/field-compression"
+repository = "https://github.com/ecmwf/field-compression.git"
+
+[project.scripts]
+fcpy = "fcpy.cli:main"
+
+[build-system]
+requires = ["setuptools>=42", "wheel"]
+build-backend = "setuptools.build_meta"
diff --git a/packages/fcpy/meta.yaml b/packages/fcpy/meta.yaml
new file mode 100644
index 00000000..a5f37379
--- /dev/null
+++ b/packages/fcpy/meta.yaml
@@ -0,0 +1,28 @@
+package:
+  name: fcpy
+  version: 0.21.1
+  top-level:
+    - fcpy
+source:
+  path: fcpy
+test:
+  imports:
+    - fcpy
+requirements:
+  run:
+    - cfgrib
+    - Cartopy
+    - dask
+    - kneed
+    - matplotlib
+    - numpy
+    - scikit-image
+    - scikit-learn
+    - scipy
+    - tqdm
+    - xarray
+
+about:
+  home: https://github.com/ecmwf-lab/field-compression
+  summary: ECMWF Field Compression Laboratory
+  license: Apache-2.0
diff --git a/packages/findlibs/meta.yaml b/packages/findlibs/meta.yaml
new file mode 100644
index 00000000..0d499a42
--- /dev/null
+++ b/packages/findlibs/meta.yaml
@@ -0,0 +1,13 @@
+package:
+  name: findlibs
+  version: 0.0.5
+  top-level:
+    - PUT_TOP_LEVEL_IMPORT_NAMES_HERE
+source:
+  url: https://files.pythonhosted.org/packages/ae/be/6c72ef9d990cd25fe3dd97ebe9d77a859f7d27b7273e62ad750846d207ee/findlibs-0.0.5.tar.gz
+  sha256: 7a801571e999d0ee83f9b92cbb598c21f861ee26ca9dba74cea8958ba4335e7e
+about:
+  home: https://github.com/ecmwf/findlibs
+  PyPI: https://pypi.org/project/findlibs
+  summary: A packages to search for shared libraries on various platforms
+  license: Apache License Version 2.0
diff --git a/packages/importlib_metadata/meta.yaml b/packages/importlib_metadata/meta.yaml
new file mode 100644
index 00000000..081bb2cf
--- /dev/null
+++ b/packages/importlib_metadata/meta.yaml
@@ -0,0 +1,16 @@
+package:
+  name: importlib_metadata
+  version: 6.6.0
+  top-level:
+    - importlib_metadata
+source:
+  url: https://files.pythonhosted.org/packages/30/bb/bf2944b8b88c65b797acc2c6a2cb0fb817f7364debf0675792e034013858/importlib_metadata-6.6.0-py3-none-any.whl
+  sha256: 43dd286a2cd8995d5eaef7fee2066340423b818ed3fd70adf0bad5f1fac53fed
+requirements:
+  run:
+    - zipp
+about:
+  home: https://github.com/python/importlib_metadata
+  PyPI: https://pypi.org/project/importlib_metadata
+  summary: Read metadata from Python packages
+  license: "Apache Software License"
diff --git a/packages/kneed/meta.yaml b/packages/kneed/meta.yaml
new file mode 100644
index 00000000..6935a293
--- /dev/null
+++ b/packages/kneed/meta.yaml
@@ -0,0 +1,17 @@
+package:
+  name: kneed
+  version: 0.8.3
+  top-level:
+    - kneed
+source:
+  url: https://files.pythonhosted.org/packages/22/55/f42149c6d3ff9eed1808a1741b0e814202d12af1ddb87765f879c3b32703/kneed-0.8.3-py3-none-any.whl
+  sha256: 8d31366cbd5af929057d4c8701f0eae96460cb995dfc6306ae3183051086a82a
+requirements:
+  run:
+    - numpy
+    - scipy
+about:
+  home: https://github.com/arvkevi/kneed
+  PyPI: https://pypi.org/project/kneed
+  summary: Knee-point detection in Python
+  license: BSD-3-Clause
diff --git a/packages/libeccodes/meta.yaml b/packages/libeccodes/meta.yaml
new file mode 100644
index 00000000..4b30089c
--- /dev/null
+++ b/packages/libeccodes/meta.yaml
@@ -0,0 +1,41 @@
+package:
+  name: libeccodes
+  version: 2.30.0
+
+source:
+  url: https://github.com/ecmwf/eccodes/archive/refs/tags/2.30.0.tar.gz
+  sha256: 5869817b76d1fd3b4b4d6ec3629231e4470f52a5abbfcec003974c1a7fdecbee
+  patches:
+    - patches/0001-32bit-support-hack.patch
+
+build:
+  type: shared_library
+  script: |
+    git clone --branch 3.7.0 --depth 1 https://github.com/ecmwf/ecbuild;
+
+    mkdir -p build;
+
+    cd build \
+        && emcmake cmake ../ \
+        -DCMAKE_INSTALL_PREFIX=${WASM_LIBRARY_DIR} \
+        -DBUILD_SHARED_LIBS=ON \
+        -DENABLE_NETCDF=OFF \
+        -DENABLE_JPG=OFF \
+        -DENABLE_PNG=OFF \
+        -DENABLE_AEC=OFF \
+        -DENABLE_FORTRAN=OFF \
+        -DDISABLE_OS_CHECK=ON \
+        -DENABLE_TESTS=OFF \
+        -DENABLE_PRODUCT_GRIB=ON \
+        -DENABLE_PRODUCT_BUFR=ON \
+        -DENABLE_EXAMPLES=OFF \
+        -DENABLE_BUILD_TOOLS=OFF \
+        -DENABLE_INSTALL_ECCODES_DEFINITIONS=ON \
+        -DENABLE_INSTALL_ECCODES_SAMPLES=OFF;
+        #-DCMAKE_C_FLAGS="-s TOTAL_MEMORY=50790400" \
+        #-DCMAKE_CXX_FLAGS="-s TOTAL_MEMORY=50790400";
+
+    emmake make -j ${PYODIDE_JOBS:-3};
+    emmake make -j ${PYODIDE_JOBS:-3} install;
+
+    cp -P ${WASM_LIBRARY_DIR}/lib/libeccodes.so ${DISTDIR}
diff --git a/packages/libeccodes/patches/0001-32bit-support-hack.patch b/packages/libeccodes/patches/0001-32bit-support-hack.patch
new file mode 100644
index 00000000..3d270618
--- /dev/null
+++ b/packages/libeccodes/patches/0001-32bit-support-hack.patch
@@ -0,0 +1,83 @@
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index f7085bcae..d495de0f5 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -66,9 +66,9 @@ ecbuild_debug("ECCODES_LITTLE_ENDIAN=${ECCODES_LITTLE_ENDIAN}")
+ ecbuild_info("Operating system=${CMAKE_SYSTEM} (${EC_OS_BITS} bits)")
+ 
+ # Only support 64 bit operating systems
+-if( NOT EC_OS_BITS EQUAL "64" )
+-    ecbuild_critical( "Operating system ${CMAKE_SYSTEM} (${EC_OS_BITS} bits) -- ecCodes only supports 64 bit platforms" )
+-endif()
++# if( NOT EC_OS_BITS EQUAL "64" )
++#     ecbuild_critical( "Operating system ${CMAKE_SYSTEM} (${EC_OS_BITS} bits) -- ecCodes only supports 64 bit platforms" )
++# endif()
+ 
+ ###############################################################################
+ # some variables/options of this project
+@@ -423,7 +423,7 @@ if( HAVE_BUILD_TOOLS )
+ endif()
+ add_subdirectory( fortran )
+ 
+-add_subdirectory( tests )
++# add_subdirectory( tests )
+ add_subdirectory( examples )
+ add_subdirectory( data )
+ add_subdirectory( samples )
+diff --git a/src/grib_accessor_class_data_g1second_order_general_extended_packing.cc b/src/grib_accessor_class_data_g1second_order_general_extended_packing.cc
+index df99452a0..92e4fe6af 100644
+--- a/src/grib_accessor_class_data_g1second_order_general_extended_packing.cc
++++ b/src/grib_accessor_class_data_g1second_order_general_extended_packing.cc
+@@ -215,6 +215,15 @@ static void init_class(grib_accessor_class* c)
+ #define MAX_NUMBER_OF_GROUPS 65534
+ #define EFDEBUG 0
+ 
++static const size_t nbits[32]={
++        0x1, 0x2, 0x4, 0x8, 0x10, 0x20,
++        0x40, 0x80, 0x100, 0x200, 0x400, 0x800,
++        0x1000, 0x2000, 0x4000, 0x8000, 0x10000, 0x20000,
++        0x40000, 0x80000, 0x100000, 0x200000, 0x400000, 0x800000,
++        0x1000000, 0x2000000, 0x4000000, 0x8000000, 0x10000000, 0x20000000,
++        0x40000000, 0x80000000
++};
++/*
+ static const unsigned long nbits[64] = {
+     0x1, 0x2, 0x4, 0x8,
+     0x10, 0x20, 0x40, 0x80,
+@@ -233,6 +242,7 @@ static const unsigned long nbits[64] = {
+     0x100000000000000, 0x200000000000000, 0x400000000000000, 0x800000000000000,
+     0x1000000000000000, 0x2000000000000000, 0x4000000000000000, 0x8000000000000000
+ };
++*/
+ 
+ static long number_of_bits(grib_handle* h, unsigned long x)
+ {
+diff --git a/src/grib_accessor_class_second_order_bits_per_value.cc b/src/grib_accessor_class_second_order_bits_per_value.cc
+index 5cd3b970c..63f713fd5 100644
+--- a/src/grib_accessor_class_second_order_bits_per_value.cc
++++ b/src/grib_accessor_class_second_order_bits_per_value.cc
+@@ -151,7 +151,6 @@ static void init_class(grib_accessor_class* c)
+ 
+ /* END_CLASS_IMP */
+ 
+-/*
+ static const size_t nbits[32]={
+         0x1, 0x2, 0x4, 0x8, 0x10, 0x20,
+         0x40, 0x80, 0x100, 0x200, 0x400, 0x800,
+@@ -160,7 +159,7 @@ static const size_t nbits[32]={
+         0x1000000, 0x2000000, 0x4000000, 0x8000000, 0x10000000, 0x20000000,
+         0x40000000, 0x80000000
+ };
+-*/
++/*
+ static const size_t nbits[64] = {
+     0x1, 0x2, 0x4, 0x8,
+     0x10, 0x20, 0x40, 0x80,
+@@ -179,6 +178,7 @@ static const size_t nbits[64] = {
+     0x100000000000000, 0x200000000000000, 0x400000000000000, 0x800000000000000,
+     0x1000000000000000, 0x2000000000000000, 0x4000000000000000, 0x8000000000000000
+ };
++*/
+ 
+ static int number_of_bits(size_t x, long* result)
+ {
diff --git a/packages/libproj/meta.yaml b/packages/libproj/meta.yaml
index 9deee8d6..9a9a7060 100644
--- a/packages/libproj/meta.yaml
+++ b/packages/libproj/meta.yaml
@@ -5,6 +5,8 @@ package:
 source:
   sha256: 76ed3d0c3a348a6693dfae535e5658bbfd47f71cb7ff7eb96d9f12f7e068b1cf
   url: https://download.osgeo.org/proj/proj-8.2.1.tar.gz
+  patches:
+    - patches/0001-stod-empty-zero.patch
 
 requirements:
   host:
diff --git a/packages/libproj/patches/0001-stod-empty-zero.patch b/packages/libproj/patches/0001-stod-empty-zero.patch
new file mode 100644
index 00000000..035525dc
--- /dev/null
+++ b/packages/libproj/patches/0001-stod-empty-zero.patch
@@ -0,0 +1,16 @@
+diff --git a/src/iso19111/internal.cpp b/src/iso19111/internal.cpp
+index 4810202d..f7c667bc 100644
+--- a/src/iso19111/internal.cpp
++++ b/src/iso19111/internal.cpp
+@@ -242,6 +242,11 @@ bool ends_with(const std::string &str, const std::string &suffix) noexcept {
+ double c_locale_stod(const std::string &s) {
+ 
+     const auto s_size = s.size();
++    // Propagate <cstdlib>'s strtod behaviour
++    if (s_size == 0) {
++        return 0.0;
++    }
++
+     // Fast path
+     if (s_size > 0 && s_size < 15) {
+         std::int64_t acc = 0;
diff --git a/packages/pyshp/meta.yaml b/packages/pyshp/meta.yaml
new file mode 100644
index 00000000..355a6426
--- /dev/null
+++ b/packages/pyshp/meta.yaml
@@ -0,0 +1,13 @@
+package:
+  name: pyshp
+  version: 2.3.1
+  top-level:
+    - shapefile
+source:
+  url: https://files.pythonhosted.org/packages/98/2f/68116db5b36b895c0450e3072b8cb6c2fac0359279b182ea97014d3c8ac0/pyshp-2.3.1-py2.py3-none-any.whl
+  sha256: 67024c0ccdc352ba5db777c4e968483782dfa78f8e200672a90d2d30fd8b7b49
+about:
+  home: https://github.com/GeospatialPython/pyshp
+  PyPI: https://pypi.org/project/pyshp
+  summary: Pure Python read/write support for ESRI Shapefile format
+  license: MIT
diff --git a/packages/zipp/meta.yaml b/packages/zipp/meta.yaml
new file mode 100644
index 00000000..0aa4ec79
--- /dev/null
+++ b/packages/zipp/meta.yaml
@@ -0,0 +1,13 @@
+package:
+  name: zipp
+  version: 3.15.0
+  top-level:
+    - zipp
+source:
+  url: https://files.pythonhosted.org/packages/5b/fa/c9e82bbe1af6266adf08afb563905eb87cab83fde00a0a08963510621047/zipp-3.15.0-py3-none-any.whl
+  sha256: 48904fc76a60e542af151aded95726c1a5c34ed43ab4134b597665c86d7ad556
+about:
+  home: https://github.com/jaraco/zipp
+  PyPI: https://pypi.org/project/zipp
+  summary: Backport of pathlib-compatible object wrapper for zip files
+  license: "MIT"
